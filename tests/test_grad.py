"""
Test gradients generated by Myia (first order and second order).
"""

from myia.validate import analysis, NoTestGrad
from pytest import mark, fail
from myia.impl.impl_interp import fit, shape, sum, exp, log
from myia.front import myia
import numpy


rng = numpy.random.RandomState(138)
N = rng.randn
xfail = mark.xfail


# Pre-create random arrays M11, M12, ... N11, ... O11, ... P11, ...
# From seeds that only depend on the dimensions, so that reordering tests
# doesn't change the input values.
for i in range(1, 6):
    globals()[f'Z{i}'] = numpy.zeros((i,))
    globals()[f'O{i}'] = numpy.ones((i,))
    rng = numpy.random.RandomState(i * 1000)
    for pfx in 'MNPQ':
        globals()[f'{pfx}{i}'] = rng.randn(i)
    for j in range(1, 6):
        globals()[f'Z{i}{j}'] = numpy.zeros((i, j))
        globals()[f'O{i}{j}'] = numpy.ones((i, j))
        rng = numpy.random.RandomState(i * 1000 + j)
        for pfx in 'MNPQ':
            globals()[f'{pfx}{i}{j}'] = rng.randn(i, j)


def o(*dims):
    return numpy.ones(dims)


def grad_test(*tests):
    """
    Test the gradient of a function. This performs the following
    tests:

    1.  When applied on the given arguments, all of the following
        function calls must return the same value:
        a. The original, pure Python function.
        b. The myia-compiled function.
        c. The grad-transformed myia function.

    2.  The gradient on each input, given an output gradient of
        one, must be within a small error of the symmetric
        finite-difference estimation:
        * diff_dx = f(..., x + eps, ...) - f(..., x - eps, ...) / 2eps
        * computed_dx = J(f)(..., x, ...)[1](1)
        * error_dx = abs(diff_dx - computed_dx) > eps_2

    TODO: allow specifying the gradient values explicitly.
    """
    def decorate(fn):
        try:
            exc = None
            testfn = analysis('grad', fn).test
        except Exception as e:
            exc = e

        def test(test_data):
            if exc:
                raise exc
            results = testfn(test_data)
            print(results)
            if not results['match']:
                for row in ['python', 'myia', 'myiag']:
                    print(f'{row}:\t{results[row+"_result"]}')
                fail('Mismatch is output values (see stdout)')
            for arg, d in results["derivatives"].items():
                if not d['match']:
                    print(f'Argument {arg}:')
                    print(f'\tFinite differences: {d["difference"]}')
                    print(f'\tGradient output:    {d["exact"]}')
                    fail(f'Mismatch in gradients for {arg} (see stdout)')

        m = mark.parametrize('test_data', list(tests))(test)
        m.__orig__ = fn
        return m
    return decorate


def grad2_test(*tests):
    """
    Test the second-order gradient of a function. This currently only
    works for functions that take a single argument, but that will be
    fixed soon.
    """
    def decorate(fn):
        try:
            exc = None
            testfn = analysis('grad2', fn).test
        except Exception as e:
            exc = e

        def test(test_data):
            if exc:
                raise exc
            results = testfn(test_data)
            print(results)
            for arg, d in results.items():
                if not d['match']:
                    print(f'Argument {arg}:')
                    print(f'\tFinite differences: {d["difference"]}')
                    print(f'\tGradient output:    {d["computed"]}')
                    fail(f'Mismatch in gradients for {arg} (see stdout)')

        m = mark.parametrize('test_data', list(tests))(test)
        m.__orig__ = fn
        return m
    return decorate


#########################
# First-order gradients #
#########################

@grad_test((13, 14))
def test_null(x, y):
    """Test null gradient."""
    return 10 + 28 / 43


@grad_test((3, 4))
def test_tuple(x, y):
    """Test multiple outputs."""
    return (x + y, x - y, x * y, x / y)


@grad_test((3, 4, 5))
def test_expression(x, y, z):
    """Test a more complex expression."""
    return x * y + y / z


@grad_test((3, 4, 5))
def test_variables(x, y, z):
    """Test an expression with temporary variables."""
    a = x * y
    b = y * a
    c = a + b
    return c / z


@grad_test((3, 4, 5))
def test_shadowing(x, y, z):
    """Test an expression where variables are shadowed."""
    x = x * y
    y = y * z
    z = x * z
    return x + y + z


@grad_test((3,))
def test_constant(x):
    """Test the use of a literal in the expression."""
    return 18 * x


@grad_test((3,))
def test_dup_args_in_call(x):
    """The naive gradient update rule fails when a function's arguments
    contain the same variable more than once."""
    return x * x


@grad_test((3,))
def test_quadruple_args_in_call(x):
    """Test that duplicated arguments still cause no problem even if
    there are four of them."""
    def g(a, b, c, d):
        return a * b * c * d
    return g(x, x, x, x)


@grad_test((4, 5))
def test_simple_closure(a, b):
    """Test some trivial closures."""
    def f():
        return a + 1

    def g():
        return b + 2
    return f() * g()


@grad_test((4,))
def test_closure(a):
    """This is the closure test in the paper."""
    def x1(b):

        def x4(c):
            return b
        return x4
    x2 = x1(a)
    x3 = x2(1)
    return x3


# TODO: test when a == b (finite diff won't return anything sensible)
@grad_test((4, 5), (68, -4))
def test_if(a, b):
    # This is max, but what this is really testing is the most basic
    # if statement, so I prefer to name the test 'test_if'
    if a > b:
        return a
    else:
        return b


@grad_test((4, 5, 2), (7, 3, 1))
def test_while(x, y, z):
    rval = 0
    # Cannot compare to 0 or finite diff is unstable
    while x > -0.1:
        rval += y
        x -= z
    return rval


@grad_test((2,), (4,), (8,))
def test_for(n):
    y = 0
    for x in range(10):
        y += n
    return y


@grad_test((2,), (3,))
def test_pow10(x):
    v = x
    i = 0
    j = 0
    while j < 3:
        i = 0
        while i < 3:
            v = v * x
            i = i + 1
        j = j + 1
    return v


##############################
# Test particular operations #
##############################


@grad_test((M33, N33))
def test_add(x, y):
    return x + y


@grad_test((M33, N33))
def test_subtract(x, y):
    return x - y


@grad_test((M33, N33))
def test_multiply(x, y):
    return x * y


@grad_test((M33, N33))
def test_divide(x, y):
    return x / y


# The first test outputs a numpy warning because Myia calculates the gradient
# wrt the exponent (3), so it tries to compute log(-2).
# The second test squares the numbers to ensure they are positive, so all
# gradients are well-defined.
@grad_test((-2, NoTestGrad(3)), (M33**2, N33))
def test_pow(x, y):
    return x ** y


@grad_test((M33,))
def test_exp(x):
    return exp(x)


@grad_test((M33 ** 2,))
def test_log(x):
    return log(x)


@grad_test((M34, M42), (M51, M15), (M15, M51))
def test_dot(x, y):
    return x @ y


@grad_test((M55,), (M15,), (M51,))
def test_sum(x):
    return sum(x)


@grad_test((M55, M15), (M15, M55), (M55, M11), (M51, M15),
           (M5, M55), (M55, M5))
def test_fit(x, y):
    return fit(x, shape(y))


##########################
# Second-order gradients #
##########################


@grad2_test((12,))
def test_g2_simple(x):
    return x * x * x * x


@grad2_test((3,))
def test_g2_loop(x):
    # Unfortunately, for the time being, this test is slow as balls,
    # because grad2 is slow as balls.
    rval = 2
    y = 3
    while y > -0.1:
        rval *= x
        y -= 1
    return rval


@grad2_test((5,))
def test_g2_range(n):
    # Even slower :(
    y = 0
    for x in range(10):
        y += x * n * n * n
    return y


#######################
# Logistic regression #
#######################


def sigmoid(x):
    return 1 / (1 + exp(-x))


def logistic_regression(params, x):
    w, b = params
    act = x @ w + b
    return sigmoid(act)


@grad_test(((M51, Z11), M15, M11),
           ((N31, N11), N13, N11))
def test_cost(params, x, y):
    y_hat = logistic_regression(params, x)
    return sum((y_hat - y) ** 2)
